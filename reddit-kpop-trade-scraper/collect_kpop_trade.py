#!/usr/bin/env python3
"""
ğŸµ K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

Redditì—ì„œ K-pop ì•„ì´ëŒ í¬í† ì¹´ë“œ WTS/WTB/WTT ê±°ë˜ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python collect_kpop_trade.py                      # ì„¸ë¸í‹´ ê¸°ë³¸ ìˆ˜ì§‘
    python collect_kpop_trade.py --artist "BTS"       # ë‹¤ë¥¸ ì•„ì´ëŒ
    python collect_kpop_trade.py --limit 50           # ìˆ˜ì§‘ ê°œìˆ˜ ì¡°ì •
"""

import argparse
import json
import os
import sys
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import List, Optional

import requests
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ============================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================

class SearchSource(str, Enum):
    """ê²€ìƒ‰ ì†ŒìŠ¤"""
    REDDIT = "reddit"
    YOUTUBE = "youtube"
    TWITTER = "twitter"
    WEB = "web"


class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""
    url: str = Field(..., description="ê²°ê³¼ URL")
    title: str = Field(..., description="ì œëª©")
    snippet: str = Field(..., description="ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°")
    source: SearchSource = Field(..., description="ê²€ìƒ‰ ì†ŒìŠ¤")
    lang: str = Field(..., description="ì–¸ì–´ ì½”ë“œ")
    queried_at: datetime = Field(default_factory=datetime.now, description="ê²€ìƒ‰ ì‹œê°„")


# ============================================================
# SerpAPI ê²€ìƒ‰ í´ë˜ìŠ¤
# ============================================================

class SerpSearcher:
    """SerpAPI ê¸°ë°˜ ê²€ìƒ‰ í´ë˜ìŠ¤"""

    def __init__(self, api_key: Optional[str] = None, output_dir: str = "data"):
        self.api_key = api_key or os.getenv("SERPAPI_KEY")
        if not self.api_key:
            raise ValueError("âŒ SERPAPI_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        self.base_url = "https://serpapi.com/search"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def build_query(self, keywords: List[str], source: SearchSource) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        keyword_query = " AND ".join(keywords)
        
        if source == SearchSource.REDDIT:
            return f"{keyword_query} site:reddit.com"
        elif source == SearchSource.YOUTUBE:
            return f"{keyword_query} site:youtube.com"
        elif source == SearchSource.TWITTER:
            return f"{keyword_query} (site:x.com OR site:twitter.com)"
        return keyword_query

    @retry(
        retry=retry_if_exception_type((requests.exceptions.RequestException, TimeoutError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
    )
    def _make_request(self, params: dict) -> dict:
        """API ìš”ì²­ (ìë™ ì¬ì‹œë„ í¬í•¨)"""
        response = requests.get(self.base_url, params=params, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        if "error" in data:
            raise ValueError(f"SerpAPI ì˜¤ë¥˜: {data.get('error')}")
        return data

    def search(
        self,
        keywords: List[str],
        source: SearchSource = SearchSource.TWITTER,
        language: str = "en",
        max_results: int = 10,
    ) -> List[SearchResult]:
        """ê²€ìƒ‰ ì‹¤í–‰"""
        query = self.build_query(keywords, source)
        
        params = {
            "q": query,
            "api_key": self.api_key,
            "num": min(max_results, 100),
            "hl": language,
            "gl": "kr" if language == "ko" else "us",
            "tbs": "qdr:m6",  # ìµœê·¼ 6ê°œì›”
        }

        try:
            data = self._make_request(params)
        except ValueError as e:
            if "quota" in str(e).lower() or "limit" in str(e).lower():
                print(f"âš ï¸  API í• ë‹¹ëŸ‰ ì´ˆê³¼: {e}")
                return []
            raise

        results = []
        for item in data.get("organic_results", []):
            try:
                result = SearchResult(
                    url=item.get("link", ""),
                    title=item.get("title", ""),
                    snippet=item.get("snippet", ""),
                    source=source,
                    lang=language,
                )
                results.append(result)
            except Exception as e:
                continue

        return results


# ============================================================
# ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ í•¨ìˆ˜
# ============================================================

def get_trade_keywords(artist: str) -> dict:
    """ì•„í‹°ìŠ¤íŠ¸ë³„ ê±°ë˜ í‚¤ì›Œë“œ ìƒì„±"""
    
    # ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ ë³€í˜•
    artist_lower = artist.lower()
    
    keywords = {
        'en': [
            # WTS (íŒë§¤)
            [artist, "WTS"],
            [artist, "pc"],
            [artist, "photocard", "selling"],
            ["WTS", artist, "photocard"],
            ["WTS", artist, "pc"],
            ["selling", artist, "photocard"],
            ["for sale", artist, "photocard"],
            
            # WTB (êµ¬ë§¤)
            [artist, "WTB"],
            ["WTB", artist, "photocard"],
            ["WTB", artist, "pc"],
            ["buying", artist, "photocard"],
            ["looking for", artist, "photocard"],
            ["ISO", artist, "photocard"],
            
            # WTT (êµí™˜)
            [artist, "WTT"],
            [artist, "photocard", "trading"],
            ["WTT", artist, "photocard"],
            ["trade", artist, "photocard"],
            ["trading", artist, "pc"],
            
            # Etc
            ["got scammed", artist],
            ["legit check", artist],
            
        ],
        'ko': [
            [artist, "í¬ì¹´", "íŒë§¤"],
            [artist, "í¬ì¹´", "íŠ¸ë ˆì´ë“œ"],
            [artist, "í¬ì¹´", "êµí™˜"],
            [artist, "í¬ì¹´", "ì‚¬ê¸°"],
            [artist, "í¬í† ì¹´ë“œ", "ì‚¬ê¸°"],

            [artist, "í¬í† ì¹´ë“œ", "ì–‘ë„"],
            [artist, "í¬ì¹´", "ì–‘ë„"],
            [artist, "í¬í† ì¹´ë“œ", "íŒë§¤"],
            [artist, "í¬ì¹´", "êµ¬í•´ìš”"],
            [artist, "í¬ì¹´", "ì‚½ë‹ˆë‹¤"],
            [artist, "í¬ì¹´", "íŒë‹ˆë‹¤"],
        ]
    }
    
    return keywords


def filter_trade_posts(results: List[SearchResult]) -> List[SearchResult]:
    """ê±°ë˜ ê´€ë ¨ ê²Œì‹œê¸€ë§Œ í•„í„°ë§"""
    trade_keywords = [
        'wts', 'wtb', 'wtt', 'trade', 'trading', 'selling', 'buying', 
        'for sale', 'iso', 'ì–‘ë„', 'íŒë§¤', 'êµ¬í•´', 'ì‚½ë‹ˆë‹¤', 'íŒë‹ˆë‹¤', 'êµí™˜'
    ]
    
    filtered = []
    for result in results:
        combined = (result.title + " " + result.snippet).lower()
        if any(kw in combined for kw in trade_keywords):
            filtered.append(result)
    
    return filtered


def collect_trade_posts(
    artist: str = "Seventeen", 
    limit: int = 100, 
    languages: List[str] = None,
    source: SearchSource = SearchSource.TWITTER,
    artist_case_variants: bool = False,
    ):
    """ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ ë©”ì¸ í•¨ìˆ˜"""
    
    if languages is None:
        languages = ["en"]
    
    if artist_case_variants:
        variants = []
        for a in [artist, artist.lower(), artist.upper(), artist.title()]:
            if a not in variants:
                variants.append(a)
        artist_query = "(" + " OR ".join(variants) + ")"
    else:
        artist_query = artist
    
    print("=" * 60)
    print(f"ğŸµ {artist} í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘")
    print("=" * 60)
    print(f"ğŸ¯ Target: WTS/WTB/WTT ê±°ë˜ ê²Œì‹œê¸€")
    print(f"ğŸŒ Languages: {', '.join(languages)}")
    print(f"ğŸ“Š Limit: ~{limit} posts")
    print()

    # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    try:
        searcher = SerpSearcher()
        print("âœ… SerpAPI ì—°ê²° ì„±ê³µ")
    except ValueError as e:
        print(f"âŒ {e}")
        print("ğŸ’¡ .env íŒŒì¼ì— SERPAPI_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        return None

    # í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    all_keywords = get_trade_keywords(artist_query)

    # ê²€ìƒ‰ ì‹¤í–‰
    all_results = []
    for lang in languages:
        if lang not in all_keywords:
            continue
        
        keywords_list = all_keywords[lang]
        print(f"\nğŸŒ Language: {lang.upper()} ({len(keywords_list)} í‚¤ì›Œë“œ)")
        
        for i, keywords in enumerate(keywords_list, 1):
            print(f"  [{i}/{len(keywords_list)}] {' + '.join(keywords)}")
            
            try:
                results = searcher.search(
                    keywords=keywords,
                    source=source,
                    language=lang,
                    max_results=10
                )
                print(f"    âœ… {len(results)} results")
                all_results.extend(results)
            except Exception as e:
                print(f"    âš ï¸ No results")

    # ì¤‘ë³µ ì œê±°
    seen_urls = set()
    unique_results = []
    for result in all_results:
        if result.url not in seen_urls:
            unique_results.append(result)
            seen_urls.add(result.url)
    
    print(f"\nğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(unique_results)}ê°œ")

    # ê±°ë˜ ê²Œì‹œê¸€ í•„í„°ë§
    filtered_results = filter_trade_posts(unique_results)
    print(f"ğŸ” ê±°ë˜ í‚¤ì›Œë“œ í•„í„° í›„: {len(filtered_results)}ê°œ")

    # ì œí•œ ì ìš©
    if len(filtered_results) > limit:
        filtered_results = filtered_results[:limit]

    # ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    artist_safe = artist.lower().replace(" ", "_")
    filename = Path("data") / f"{artist_safe}_trade_{timestamp}.jsonl"
    Path("data").mkdir(exist_ok=True)
    
    with open(filename, "w", encoding="utf-8") as f:
        for result in filtered_results:
            data = {
                "url": result.url,
                "title": result.title,
                "snippet": result.snippet,
                "source": result.source.value,
                "lang": result.lang,
                "queried_at": result.queried_at.isoformat(),
            }
            f.write(json.dumps(data, ensure_ascii=False) + "\n")

    print(f"\n{'=' * 60}")
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(filtered_results)}ê°œ ê±°ë˜ ê²Œì‹œê¸€")
    print(f"ğŸ’¾ ì €ì¥: {filename}")
    print("=" * 60)

    # ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“‹ ìˆ˜ì§‘ëœ ê±°ë˜ ê²Œì‹œê¸€ ìƒ˜í”Œ:")
    for i, result in enumerate(filtered_results[:10], 1):
        title = result.title[:55] + "..." if len(result.title) > 55 else result.title
        print(f"  {i}. {title}")
    
    if len(filtered_results) > 10:
        print(f"  ... ì™¸ {len(filtered_results) - 10}ê°œ")

    return filename


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python collect_kpop_trade.py                    # ì„¸ë¸í‹´ ìˆ˜ì§‘
  python collect_kpop_trade.py --artist "BTS"     # BTS ìˆ˜ì§‘
  python collect_kpop_trade.py --artist "TWICE"   # íŠ¸ì™€ì´ìŠ¤ ìˆ˜ì§‘
  python collect_kpop_trade.py --limit 50         # 50ê°œë§Œ ìˆ˜ì§‘
        """
    )

    parser.add_argument(
        "--artist",
        type=str,
        default="Seventeen",
        help="ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ (ê¸°ë³¸: Seventeen)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=100,
        help="ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸: 100)",
    )
    parser.add_argument(
        "--languages",
        type=str,
        default="en",
        help="ì–¸ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸: en)",
    )
    parser.add_argument(
        "--searchsource",
        type=str,
        default="twitter",
        choices=[s.value for s in SearchSource],
        help="ê²€ìƒ‰ ì†ŒìŠ¤ (ê¸°ë³¸: twitter)",
    )    
    parser.add_argument(
        "--artist_case_variants",
        action="store_true",
        help="artistë¥¼ Seventeen/SEVENTEEN/seventeen ë“±ìœ¼ë¡œ OR ë¬¶ì–´ì„œ í•¨ê»˜ ê²€ìƒ‰",
    )    

    args = parser.parse_args()
    languages = [l.strip() for l in args.languages.split(",")]
    
    source = SearchSource(args.searchsource.lower())

    collect_trade_posts(
        artist=args.artist, 
        limit=args.limit, 
        languages=languages,
        source=source,
        artist_case_variants=args.artist_case_variants,
        )


if __name__ == "__main__":
    main()

